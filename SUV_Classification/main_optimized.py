#!/usr/bin/env python3
"""
Script ch√≠nh s·ª≠ d·ª•ng ng∆∞·ª°ng ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u
"""

import os
import sys
import glob
import json
import numpy as np

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from suv_classifier import SUVClassifier
from evaluator import SUVEvaluator
from audio_analyzer import AudioAnalyzer

def load_optimized_thresholds(results_dir: str):
    """
    Load ng∆∞·ª°ng ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u t·ª´ file
    """
    thresholds_file = os.path.join(results_dir, "best_thresholds.json")
    
    if os.path.exists(thresholds_file):
        with open(thresholds_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print(f"Kh√¥ng t√¨m th·∫•y file ng∆∞·ª°ng t·ªëi ∆∞u: {thresholds_file}")
        print("H√£y ch·∫°y auto_optimize.py tr∆∞·ªõc ƒë·ªÉ t√¨m ng∆∞·ª°ng t·ªëi ∆∞u!")
        return None

def main():
    """
    Ch·∫°y ph√¢n lo·∫°i SUV v·ªõi ng∆∞·ª°ng ƒë√£ t·ªëi ∆∞u
    """
    print("=== SUV CLASSIFICATION V·ªöI NG∆Ø·ª†NG T·ªêI ·ª¨U ===\\n")
    
    # C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
    base_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(base_dir)
    
    # Th∆∞ m·ª•c d·ªØ li·ªáu
    training_data_dir = os.path.join(project_root, "Thi gi·ªØa k·ª≥", "TinHieuHuanLuyen")
    
    # Th∆∞ m·ª•c k·∫øt qu·∫£
    results_dir = os.path.join(project_root, "results")
    os.makedirs(results_dir, exist_ok=True)
    
    # Load ng∆∞·ª°ng t·ªëi ∆∞u
    optimized_params = load_optimized_thresholds(results_dir)
    if optimized_params is None:
        print("Ch·∫°y ch·∫ø ƒë·ªô t·ªëi ∆∞u m·∫∑c ƒë·ªãnh...")
        # Fallback: s·ª≠ d·ª•ng main.py g·ªëc
        from main import main as original_main
        original_main()
        return
    
    print("SU DUNG NGUONG DA TOI UU:")
    print(f"   Frame: {optimized_params['frame_length']*1000:.0f}ms/{optimized_params['frame_shift']*1000:.0f}ms")
    
    # L·∫•y ng∆∞·ª°ng (c√≥ th·ªÉ l√† c·∫•u tr√∫c c≈© ho·∫∑c m·ªõi)
    energy_thresh = optimized_params.get('energy_threshold', optimized_params.get('ste_speech_silence', 0))
    zcr_thresh = optimized_params.get('zcr_threshold', optimized_params.get('zcr_voiced_unvoiced', 0))
    st_thresh = optimized_params.get('st_threshold', optimized_params.get('st_voiced_unvoiced', 0.7))
    
    print(f"   Energy Threshold: {energy_thresh:.6f} (STE cho silence vs speech)")
    print(f"   ZCR Threshold: {zcr_thresh:.6f} (ZCR cho voiced vs unvoiced)")
    print(f"   ST Threshold: {st_thresh:.6f} (Spectrum Tilt cho voiced vs unvoiced)")
    print(f"   Optimization Score: {optimized_params['score']:.4f}")
    
    print(f"\\nLOGIC SUVDA (FIXED - TU BAI BAO, CHI TOI UU NGUONG):")
    print(f"   SILENCE: STE < {energy_thresh:.4f} AND ZCR < {zcr_thresh:.4f}")
    print(f"   VOICED: STE >= {energy_thresh:.4f} AND ST > {st_thresh:.4f} AND ZCR < {zcr_thresh:.4f}")
    print(f"   UNVOICED: STE >= {energy_thresh:.4f} AND ST < {st_thresh:.4f} AND ZCR > {zcr_thresh:.4f}\\n")
    
    # Ki·ªÉm tra th∆∞ m·ª•c d·ªØ li·ªáu
    if not os.path.exists(training_data_dir):
        print(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c d·ªØ li·ªáu: {training_data_dir}")
        return
    
    # L·∫•y danh s√°ch file training
    training_files = []
    wav_files = glob.glob(os.path.join(training_data_dir, "*.wav"))
    
    for wav_file in wav_files:
        lab_file = wav_file.replace('.wav', '.lab')
        if os.path.exists(lab_file):
            training_files.append((wav_file, lab_file))
    
    if len(training_files) == 0:
        print("Kh√¥ng t√¨m th·∫•y file training n√†o!")
        return
    
    print(f"T√¨m th·∫•y {len(training_files)} file training:")
    for wav_path, lab_path in training_files:
        print(f"  - {os.path.basename(wav_path)}")
    print()
    
    # Kh·ªüi t·∫°o classifier v·ªõi params t·ªëi ∆∞u
    classifier = SUVClassifier(
        frame_length=optimized_params['frame_length'],
        frame_shift=optimized_params['frame_shift'],
        sr=16000
    )
    
    # Set ng∆∞·ª°ng t·ªëi ∆∞u SUVDA (3 ng∆∞·ª°ng)
    energy_thresh = optimized_params.get('energy_threshold', optimized_params.get('ste_speech_silence', 0))
    zcr_thresh = optimized_params.get('zcr_threshold', optimized_params.get('zcr_voiced_unvoiced', 0))
    st_thresh = optimized_params.get('st_threshold', optimized_params.get('st_voiced_unvoiced', 0.7))
    
    classifier.ste_thresholds = {
        'speech_silence': energy_thresh,
        'voiced_unvoiced': 0  # Kh√¥ng d√πng trong logic SUVDA
    }
    classifier.zcr_thresholds = {
        'speech_silence': 0,  # Kh√¥ng d√πng trong logic SUVDA
        'voiced_unvoiced': zcr_thresh
    }
    # Kh·ªüi t·∫°o ST thresholds (n·∫øu ch∆∞a c√≥)
    if not hasattr(classifier, 'st_thresholds'):
        classifier.st_thresholds = {}
    classifier.st_thresholds = {
        'speech_silence': 0,  # Kh√¥ng d√πng trong logic SUVDA
        'voiced_unvoiced': st_thresh
    }
    classifier.trained = True  # Skip training v√¨ ƒë√£ c√≥ ng∆∞·ª°ng
    
    # Kh·ªüi t·∫°o evaluator v√† analyzer v·ªõi params t·ªëi ∆∞u
    evaluator = SUVEvaluator(sr=16000, hop_size=optimized_params['frame_shift'])
    analyzer = AudioAnalyzer(
        frame_length=optimized_params['frame_length'],
        frame_shift=optimized_params['frame_shift'],
        sr=16000
    )
    
    print("=== ƒê√ÅNH GI√Å V·ªöI NG∆Ø·ª†NG T·ªêI ·ª¨U ===")
    evaluation_results = []
    
    for i, (wav_path, lab_path) in enumerate(training_files):
        filename = os.path.basename(wav_path)
        print(f"\\nX·ª≠ l√Ω file {i+1}/{len(training_files)}: {filename}")
        
        # Ph√¢n lo·∫°i v·ªõi ng∆∞·ª°ng t·ªëi ∆∞u (bao g·ªìm ST)
        result = classifier.classify(wav_path)
        if len(result) == 5:  # C√≥ ST
            audio, ste, zcr, st, predictions = result
        else:  # Fallback
            audio, ste, zcr, predictions = result
            st = None
        
        # L√†m m·ªãn k·∫øt qu·∫£
        smoothed_predictions = classifier.smooth_predictions(predictions, min_segment_length=30)
        
        # Load ground truth
        segments = analyzer.load_labels(lab_path)
        true_labels = analyzer.get_frame_labels(segments, len(audio))
        
        # ƒê·∫£m b·∫£o chi·ªÅu d√†i kh·ªõp nhau
        min_length = min(len(true_labels), len(smoothed_predictions))
        true_labels = true_labels[:min_length]
        smoothed_predictions = smoothed_predictions[:min_length]
        
        # T√≠nh to√°n boundaries
        true_boundaries = evaluator.segments_to_boundaries(segments)
        pred_boundaries = evaluator.predictions_to_boundaries(smoothed_predictions)
        
        # ƒê√°nh gi√°
        boundary_metrics = evaluator.compute_boundary_error(true_boundaries, pred_boundaries)
        frame_metrics = evaluator.compute_frame_accuracy(true_labels, smoothed_predictions)
        
        print(f"  Boundary Error - MAE: {boundary_metrics['mae']:.4f}s, RMSE: {boundary_metrics['rmse']:.4f}s")
        print(f"  Frame Accuracy: {frame_metrics['overall_accuracy']:.4f}")
        print(f"  Class Accuracies - Silence: {frame_metrics['class_accuracies']['silence']:.4f}, " + 
              f"Voiced: {frame_metrics['class_accuracies']['voiced']:.4f}, " +
              f"Unvoiced: {frame_metrics['class_accuracies']['unvoiced']:.4f}")
        
        # L∆∞u k·∫øt qu·∫£
        result = {
            'filename': filename,
            'boundary_metrics': boundary_metrics,
            'frame_metrics': frame_metrics
        }
        evaluation_results.append(result)
        
        # V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì (SUVDA v·ªõi ST)
        plot_title = f"SUV Classification (SUVDA Optimized) - {filename.replace('.wav', '')}"
        plot_path = os.path.join(results_dir, f"{filename.replace('.wav', '')}_suvda_result.png")
        
        evaluator.plot_results(
            audio=audio,
            ste=ste,
            zcr=zcr,
            true_labels=true_labels,
            pred_labels=smoothed_predictions,
            true_boundaries=true_boundaries,
            pred_boundaries=pred_boundaries,
            title=plot_title,
            save_path=plot_path,
            st=st  # Th√™m Spectrum Tilt
        )
    
    # T·∫°o b√°o c√°o t·ªïng h·ª£p
    print("\\n=== B√ÅO C√ÅO K·∫æT QU·∫¢ V·ªöI NG∆Ø·ª†NG T·ªêI ·ª¨U ===")
    
    # Th·ªëng k√™ t·ªïng h·ª£p
    accuracies = [r['frame_metrics']['overall_accuracy'] for r in evaluation_results]
    maes = [r['boundary_metrics']['mae'] for r in evaluation_results]
    rmses = [r['boundary_metrics']['rmse'] for r in evaluation_results]
    
    print(f"\\nTH·ªêNG K√ä HI·ªÜU SU·∫§T:")
    print(f"Frame Accuracy - Mean: {np.mean(accuracies):.4f}, Std: {np.std(accuracies):.4f}")
    print(f"Boundary MAE - Mean: {np.mean(maes):.4f}s, Std: {np.std(maes):.4f}s")
    print(f"Boundary RMSE - Mean: {np.mean(rmses):.4f}s, Std: {np.std(rmses):.4f}s")
    
    # So s√°nh v·ªõi baseline (n·∫øu c√≥)
    baseline_file = os.path.join(results_dir, "evaluation_report.txt")
    if os.path.exists(baseline_file):
        print("\\nüìä SO S√ÅNH V·ªöI BASELINE:")
        print("   (Xem chi ti·∫øt trong file b√°o c√°o)")
    
    # T·∫°o b√°o c√°o
    report_content = evaluator.generate_report(evaluation_results)
    
    optimized_report_file = os.path.join(results_dir, "optimized_evaluation_report.txt")
    with open(optimized_report_file, 'w', encoding='utf-8') as f:
        f.write("=== B√ÅO C√ÅO SUV CLASSIFICATION V·ªöI NG∆Ø·ª†NG T·ªêI ·ª¨U ===\\n\\n")
        
        f.write("THAM S·ªê T·ªêI ·ª¨U S·ª¨ D·ª§NG:\\n")
        f.write(f"Frame Length: {optimized_params['frame_length']*1000:.0f}ms\\n")
        f.write(f"Frame Shift: {optimized_params['frame_shift']*1000:.0f}ms\\n")
        
        energy_thresh = optimized_params.get('energy_threshold', optimized_params.get('ste_speech_silence', 0))
        zcr_thresh = optimized_params.get('zcr_threshold', optimized_params.get('zcr_voiced_unvoiced', 0))
        st_thresh = optimized_params.get('st_threshold', optimized_params.get('st_voiced_unvoiced', 0.7))
        
        f.write(f"üéØ Energy Threshold: {energy_thresh:.6f} (STE cho silence vs speech)\\n")
        f.write(f"üéØ ZCR Threshold: {zcr_thresh:.6f} (ZCR cho voiced vs unvoiced)\\n")
        f.write(f"üÜï ST Threshold: {st_thresh:.6f} (Spectrum Tilt cho voiced vs unvoiced)\\n")
        f.write(f"Optimization Score: {optimized_params['score']:.4f}\\n")
        
        f.write(f"\\nLOGIC SUVDA (3 ƒê·∫∂C TR∆ØNG):\\n")
        f.write(f"1. SILENCE: STE < {energy_thresh:.6f} AND ZCR < {zcr_thresh:.6f}\\n")
        f.write(f"2. VOICED: STE ‚â• {energy_thresh:.6f} AND ST > {st_thresh:.6f} AND ZCR < {zcr_thresh:.6f}\\n")
        f.write(f"3. UNVOICED: STE ‚â• {energy_thresh:.6f} AND ST < {st_thresh:.6f} AND ZCR > {zcr_thresh:.6f}\\n\\n")
        
        f.write(report_content)
    
    print(f"\\nƒê√£ l∆∞u b√°o c√°o v√†o: {optimized_report_file}")
    print(f"\\n‚úÖ HO√ÄN TH√ÄNH ƒê√ÅNH GI√Å V·ªöI NG∆Ø·ª†NG T·ªêI ·ª¨U!")
    print(f"üìà Frame Accuracy trung b√¨nh: {np.mean(accuracies):.4f}")
    print(f"üìä T·∫•t c·∫£ k·∫øt qu·∫£ trong th∆∞ m·ª•c: {results_dir}")

if __name__ == "__main__":
    main()
